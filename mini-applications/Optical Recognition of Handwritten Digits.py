# -*- coding: utf-8 -*-
"""ID 202000369 -Assignment 4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pMiYehsLj20xsNJguYIPe0Kn0EphFIOg
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import random

# %matplotlib inline
from matplotlib import pyplot as plt

from sklearn import datasets
from sklearn import svm

from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

!pip install pillow

# Load digits from Sklearn 
dig_data = datasets.load_digits()

print(dig_data.DESCR)

# DataType of imported dataset 
print(type(dig_data))

# no. of dig_data image cases
print("Total Cases : ", len(dig_data.images))

# Keys in dig_data dataset 
# [Bunch objects are similar to dictionaries]
# Here each kay has a bunch of values linked to it
print(dig_data.keys())

# Targets of dig_data
# the .target attribute provides the target value for all the
# data entries of the dig_data dataset
print(dig_data.target)

#The Allowed targets are
print("Allowed targets : ", *dig_data.target_names)

# Image attribute values be like
dig_data.images[0]

# Visualization of first Image
plt.figure(figsize=(4, 4))
plt.imshow(dig_data.images[0], interpolation = "nearest", cmap = plt.cm.gray_r)
plt.title("Hand Written Digit")

# Reshape the image data in an accessible manner
digits = dig_data.images.reshape((len(dig_data.images), -1))

# shape of digits dataset
digits.shape

# Split the data into training set and testing set in [99:1 ratio] 
x_train, x_test, y_train, y_test = train_test_split(digits, dig_data.target, 
                                        train_size = 0.7, random_state=1)

# Create a GridSearch to find the best_parameters for the good accuracy
GSV = GridSearchCV(svm.SVC(kernel = "rbf"), 
        param_grid = {"gamma" : [0.001, 0.01, 0.1, 1], "C" : [1.0, 1.0, 100.0]}, cv=3)

GSV.fit(x_train, y_train)

GSV.cv_results_

# Select the best parameters 
# one can also look through the rank_test_score for more insights
GSV.best_params_

# Now using best parameters create a SVC mode
model = svm.SVC(C = 10.0, gamma = 0.001)
# Fit the model with training dataset
model.fit(x_train, y_train)

# Model score
score = model.score(x_test, y_test)
print(score)

# Acuraccy metrics
accuracy_score(y_test, model.predict(x_test))

# Classification report
print(classification_report(y_test, model.predict(x_test)))

# Confusion Matrix
confusion_matrix(y_test, model.predict(x_test))

# Now lets create a model with C = 100.0
# ususally the best parameters of gridScearch varies
# As random splitting takes place the 
# train and test data changes every time we run it

model = svm.SVC(C = 100.0, gamma = 0.001)
model.fit(x_train, y_train)

print(model.score(x_test, y_test))
print("Accuracy -", accuracy_score(y_test, model.predict(x_test)))

print(classification_report(y_test, model.predict(x_test)))

# Confusion matrix for C = 100
confusion_matrix(y_test, model.predict(x_test))

!pip install --upgrade scikit-learn